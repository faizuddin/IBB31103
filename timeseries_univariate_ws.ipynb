{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timeseries-univariate-ws.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMH8vzjp3SkH9pXQC2LEPL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faizuddin/IBB31103/blob/main/timeseries_univariate_ws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaayZgu-3BUn"
      },
      "source": [
        "# Timeseries Data Forecasting Using RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Xyl0raiExf"
      },
      "source": [
        "# load and plot dataset\n",
        "import pandas as pd\n",
        "\n",
        "# load dataset\n",
        "def parser(x):\n",
        "\treturn pd.datetime.strptime(\"190\"+x, \"%Y-%m\")\n",
        "series = pd.read_csv(\"https://raw.githubusercontent.com/faizuddin/workshop/main/shampoo.csv\", header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3OeLQ5hAiI"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# summarize first few rows\n",
        "print(series.head())\n",
        "\n",
        "# line plot\n",
        "series.plot()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asxOu0mKjUZK"
      },
      "source": [
        "## LSTM Data Preparation\n",
        "\n",
        "Before we can fit an LSTM model to the dataset, we must transform the data.\n",
        "\n",
        "This section is broken down into three steps:\n",
        "\n",
        "1. Transform the time series into a supervised learning problem\n",
        "2. Transform the time series data so that it is stationary.\n",
        "3. Transform the observations to have a specific scale.\n",
        "\n",
        "### Transform Time Series to Supervised Learning\n",
        "\n",
        "The LSTM model in Keras assumes that your data is divided into input (`X`) and output (`y`) components.\n",
        "\n",
        "For a time series problem, we can achieve this by using the observation from the last time step (`t-1`) as the input and the observation at the current time step (`t`) as the output.\n",
        "\n",
        "We can achieve this using the `shift()` function in Pandas that will push all values in a series down by a specified number places. We require a shift of `1` place, which will become the input variables. The time series as it stands will be the output variables.\n",
        "\n",
        "We can then concatenate these two series together to create a DataFrame ready for supervised learning. The pushed-down series will have a new position at the top with no value. A `NaN` (not a number) value will be used in this position. We will replace these `NaN` values with `0` values, which the LSTM model will have to learn as *“the start of the series”* or *“I have no data here,”* as a month with zero sales on this dataset has not been observed.\n",
        "\n",
        "The code below defines a helper function to do this called `timeseries_to_supervised()`. It takes a NumPy array of the raw time series data and a lag or number of shifted series to create and use as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T95F6ppjbgi"
      },
      "source": [
        "# frame a sequence as a supervised learning problem\n",
        "def timeseries_to_supervised(data, lag=1):\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
        "\tcolumns.append(df)\n",
        "\tdf = pd.concat(columns, axis=1)\n",
        "\tdf.fillna(0, inplace=True)\n",
        "\treturn df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wNRtVixkV39"
      },
      "source": [
        "We can test this function with our loaded Shampoo Sales dataset and convert it into a supervised learning problem.\n",
        "\n",
        "The code prints the first 5 rows of the new supervised learning problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmB5m5-NkXqO"
      },
      "source": [
        "# transform to supervised learning\n",
        "X = series.values\n",
        "supervised = timeseries_to_supervised(X, 1)\n",
        "print(supervised.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLTL-e_XlCWG"
      },
      "source": [
        "### Transform Time Series to Stationary\n",
        "\n",
        "The Shampoo Sales dataset is not **stationary**. This means that there is a structure in the data that is dependent on the time. Specifically, there is an increasing trend in the data.\n",
        "\n",
        "Stationary data is easier to model and will very likely result in more skillful forecasts.\n",
        "\n",
        "The trend can be removed from the observations, then added back to forecasts later to return the prediction to the original scale and calculate a comparable error score.\n",
        "\n",
        "A standard way to remove a trend is by differencing the data. That is the observation from the previous time step (`t-1`) is subtracted from the current observation (`t`). This removes the trend and we are left with a difference series, or the changes to the observations from one time step to the next.\n",
        "\n",
        "We can achieve this automatically using the `diff()` function in pandas. Alternatively, we can get finer grained control and write our own function to do this, which is preferred for its flexibility in this case.\n",
        "\n",
        "Below is a function called `difference()` that calculates a differenced series. Note that the first observation in the series is skipped as there is no prior observation with which to calculate a differenced value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z7OojaZkeLf"
      },
      "source": [
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "  diff = list()\n",
        "  for i in range(interval, len(dataset)):\n",
        "    value = dataset[i] - dataset[i - interval]\n",
        "    diff.append(value)\n",
        "  return pd.Series(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycm1pFNDlrFb"
      },
      "source": [
        "We also need to invert this process in order to take forecasts made on the differenced series back into their original scale.\n",
        "\n",
        "The function below, called `inverse_difference()`, inverts this operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6_zrb8TlvRw"
      },
      "source": [
        "# invert differenced value\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "\treturn yhat + history[-interval]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA4sl8fMlzOa"
      },
      "source": [
        "We can test out these functions by differencing the whole series, then returning it to the original scale, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEy3vv7fl3Ln"
      },
      "source": [
        "# transform to be stationary\n",
        "differenced = difference(series, 1)\n",
        "print(\"Difference\\n\\n\", differenced.head())\n",
        "\n",
        "# invert transform\n",
        "inverted = list()\n",
        "for i in range(len(differenced)):\n",
        "\tvalue = inverse_difference(series, differenced[i], len(series)-i)\n",
        "\tinverted.append(value)\n",
        " \n",
        "inverted = pd.Series(inverted)\n",
        "print(\"Inverse:\\n\\n\", inverted.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT6SoX-cnLxO"
      },
      "source": [
        "### Transform Time Series to Scale\n",
        "Like other neural networks, LSTMs expect data to be within the scale of the activation function used by the network.\n",
        "\n",
        "The default activation function for LSTMs is the *hyperbolic tangent* (`tanh`), which outputs values between `-1` and `1`. This is the preferred range for the time series data.\n",
        "\n",
        "To make the experiment fair, the scaling coefficients (min and max) values must be calculated on the training dataset and applied to scale the test dataset and any forecasts. This is to avoid contaminating the experiment with knowledge from the test dataset, which might give the model a small edge.\n",
        "\n",
        "We can transform the dataset to the range `[-1, 1]` using the `MinMaxScaler` class. Like other `scikit-learn` transform classes, it requires data provided in a matrix format with rows and columns. Therefore, we must reshape our `NumPy` arrays before transforming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBIqf0CmOau"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = series.values\n",
        "X = X.reshape(len(X), 1)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler = scaler.fit(X)\n",
        "scaled_X = scaler.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv1D0Hsvnl13"
      },
      "source": [
        "Again, we must invert the scale on forecasts to return the values back to the original scale so that the results can be interpreted and a comparable error score can be calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLwYU7S2nnI_"
      },
      "source": [
        "# invert transform\n",
        "inverted_X = scaler.inverse_transform(scaled_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02qhU1dFnxfm"
      },
      "source": [
        "The example below transforms the scale of the Shampoo Sales data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYSa4AS_nvAe"
      },
      "source": [
        "# transform scale\n",
        "X = series.values\n",
        "X = X.reshape(len(X), 1)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler = scaler.fit(X)\n",
        "scaled_X = scaler.transform(X)\n",
        "scaled_series = pd.Series(scaled_X[:, 0])\n",
        "print(\"Scaled\\n\\n\", scaled_series.head())\n",
        "\n",
        "# invert transform\n",
        "inverted_X = scaler.inverse_transform(scaled_X)\n",
        "inverted_series = pd.Series(inverted_X[:, 0])\n",
        "print(\"Inversed\\n\\n\", inverted_series.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfgY4JUUoKMC"
      },
      "source": [
        "## LSTM Model Development\n",
        "\n",
        "The Long Short-Term Memory network (LSTM) is a type of Recurrent Neural Network (RNN).\n",
        "\n",
        "A benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input.\n",
        "\n",
        "In Keras, this is referred to as *stateful*, and involves setting the *stateful* argument to “True” when defining an LSTM layer.\n",
        "\n",
        "By default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\n",
        "\n",
        "The LSTM layer expects input to be in a matrix with the dimensions: `[samples, time steps, features]`.\n",
        "\n",
        "* Samples: These are independent observations from the domain, typically rows of data.\n",
        "* Time steps: These are separate time steps of a given variable for a given observation.\n",
        "* Features: These are separate measures observed at the time of observation.\n",
        "\n",
        "We have some flexibility in how the Shampoo Sales dataset is framed for the network. We will keep it simple and frame the problem as each time step in the original sequence is one separate sample, with one timestep and one feature.\n",
        "\n",
        "Given that the training dataset is defined as `X` inputs and `y` outputs, it must be reshaped into the Samples/TimeSteps/Features format:\n",
        "\n",
        "```\n",
        "X, y = train[:, 0:-1], train[:, -1]\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3TYI3uwo-z1"
      },
      "source": [
        "### Input Shape, Batch Size and Neuron\n",
        "The shape of the input data must be specified in the LSTM layer using the `batch_input_shape` argument as a tuple that specifies the expected number of observations to read each batch, the number of time steps, and the number of features.\n",
        "\n",
        "The batch size is often much smaller than the total number of samples. It, along with the number of epochs, defines how quickly the network learns the data (how often the weights are updated).\n",
        "\n",
        "The final import parameter in defining the LSTM layer is the number of neurons, also called the number of memory units or blocks. This is a reasonably simple problem and a number between 1 and 5 should be sufficient.\n",
        "\n",
        "The line below creates a single LSTM hidden layer that also specifies the expectations of the input layer via the `batch_input_shape` argument.\n",
        "\n",
        "```\n",
        "from keras.layers import LSTM\n",
        "\n",
        "layer = LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGTuN7_pedy"
      },
      "source": [
        "The network requires a single neuron in the output layer with a linear activation to predict the number of shampoo sales at the next time step.\n",
        "\n",
        "Once the network is specified, it must be compiled into an efficient symbolic representation using a backend mathematical library, in our case we are using such as TensorFlow.\n",
        "\n",
        "In compiling the network, we must specify a loss function and optimization algorithm. We will use `mean_squared_error` as the loss function as it closely matches RMSE that we will are interested in, and the efficient `ADAM` optimization algorithm.\n",
        "\n",
        "Using the Sequential Keras API to define the network, the below snippet creates and compiles the network.\n",
        "\n",
        "```\n",
        "model = Sequential()\n",
        "model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pdN6_zfruiD"
      },
      "source": [
        "Once compiled, it can be fit to the training data. Because the network is stateful, we must control when the internal state is reset. Therefore, we must manually manage the training process one epoch at a time across the desired number of epochs.\n",
        "\n",
        "By default, the samples within an epoch are shuffled prior to being exposed to the network. Again, this is undesirable for the LSTM because we want the network to build up state as it learns across the sequence of observations. We can disable the shuffling of samples by setting “shuffle” to “False“.\n",
        "\n",
        "Also by default, the network reports a lot of debug information about the learning progress and skill of the model at the end of each epoch. We can disable this by setting the “verbose” argument to the level of “0“.\n",
        "\n",
        "We can then reset the internal state at the end of the training epoch, ready for the next training iteration.\n",
        "\n",
        "Below is a loop that manually fits the network to the training data:\n",
        "\n",
        "```\n",
        "for i in range(nb_epoch):\n",
        "\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "\tmodel.reset_states()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO-ytkPWr4Hf"
      },
      "source": [
        "Putting this all together, we can define a function called `fit_lstm()` that trains and returns an LSTM model. As arguments, it takes the training dataset in a supervised learning format, a batch size, a number of epochs, and a number of neurons.\n",
        "\n",
        "```\n",
        "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
        "\tX, y = train[:, 0:-1], train[:, -1]\n",
        "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
        "\tmodel.add(Dense(1))\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\tfor i in range(nb_epoch):\n",
        "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "\t\tmodel.reset_states()\n",
        "\treturn model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuMQcRoEsIS3"
      },
      "source": [
        "The batch_size must be set to `1`. This is because it must be a factor of the size of the training and test datasets.\n",
        "\n",
        "The `predict()` function on the model is also constrained by the batch size; there it must be set to `1` because we are interested in making one-step forecasts on the test data.\n",
        "\n",
        "We will not tune the network parameters in this tutorial; instead we will use the following configuration, found with a little trial and error:\n",
        "\n",
        "* Batch Size: 1\n",
        "* Epochs: 3000\n",
        "* Neurons: 4\n",
        "\n",
        "*Consider trying 1500 epochs and 1 neuron, the performance may be better?*\n",
        "\n",
        "## LSTM Forecast\n",
        "\n",
        "Once the LSTM model is fit to the training data, it can be used to make forecasts.\n",
        "\n",
        "Again, we have some flexibility. We can decide to fit the model once on all of the training data, then predict each new time step one at a time from the test data (we’ll call this the fixed approach), or we can re-fit the model or update the model each time step of the test data as new observations from the test data are made available (we’ll call this the dynamic approach).\n",
        "\n",
        "In this tutorial, we will go with the fixed approach for its simplicity, although, we would expect the dynamic approach to result in better model skill.\n",
        "\n",
        "To make a forecast, we can call the `predict()` function on the model. This requires a 3D NumPy array input as an argument. In this case, it will be an array of one value, the observation at the previous time step.\n",
        "\n",
        "The `predict()` function returns an array of predictions, one for each input row provided. Because we are providing a single input, the output will be a 2D NumPy array with one value.\n",
        "\n",
        "We can capture this behavior in a function named `forecast()` listed below. Given a fit model, a batch-size used when fitting the model (e.g. 1), and a row from the test data, the function will separate out the input data from the test row, reshape it, and return the prediction as a single floating point value.\n",
        "\n",
        "```\n",
        "def forecast(model, batch_size, row):\n",
        "\tX = row[0:-1]\n",
        "\tX = X.reshape(1, 1, len(X))\n",
        "\tyhat = model.predict(X, batch_size=batch_size)\n",
        "\treturn yhat[0,0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDzy9flAs2dK"
      },
      "source": [
        "During training, the internal state is reset after each epoch. While forecasting, we will not want to reset the internal state between forecasts. In fact, we would like the model to build up state as we forecast each time step in the test dataset.\n",
        "\n",
        "This raises the question as to what would be a good initial state for the network prior to forecasting the test dataset.\n",
        "\n",
        "In this tutorial, we will seed the state by making a prediction on all samples in the training dataset. In theory, the internal state should be set up ready to forecast the next time step.\n",
        "\n",
        "We now have all of the pieces to fit an LSTM Network model for the Shampoo Sales dataset and evaluate its performance.\n",
        "\n",
        "## Full LTSM Pipeline\n",
        "\n",
        "This will involve drawing together all of the elements from the prior sections. There are a lot of them, so let’s review:\n",
        "\n",
        "1. Load the dataset from CSV file.\n",
        "3. Transforming the data to be stationary.\n",
        "4. Transforming the data to a supervised learning problem.\n",
        "5. Transforming the data so that it has the scale -1 to 1.\n",
        "6. Fitting a stateful LSTM network model to the training data.\n",
        "7. Evaluating the static LSTM model on the test data.\n",
        "8. Report the performance of the forecasts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLPAqjqbxR85"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmcrvO8w7jj"
      },
      "source": [
        "import pandas as pd\n",
        "# date-time parsing function for loading the dataset\n",
        "def parser(x):\n",
        "\treturn pd.datetime.strptime('190'+x, '%Y-%m')\n",
        " \n",
        " # load dataset\n",
        "series = pd.read_csv(\"https://raw.githubusercontent.com/faizuddin/workshop/main/shampoo.csv\", header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geB8c9CKxZ0z"
      },
      "source": [
        "### Trending to stationary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKx_S4mgxL2L"
      },
      "source": [
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = dataset[i] - dataset[i - interval]\n",
        "\t\tdiff.append(value)\n",
        "\treturn pd.Series(diff)\n",
        " \n",
        "# invert differenced value\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "\treturn yhat + history[-interval]\n",
        "\n",
        "\n",
        "# transform data to be stationary\n",
        "raw_values = series.values\n",
        "diff_values = difference(raw_values, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbZ3Ymsnx9Ft"
      },
      "source": [
        "### Timeseries to supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PV65yk5xvMY"
      },
      "source": [
        "# frame a sequence as a supervised learning problem\n",
        "def timeseries_to_supervised(data, lag=1):\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
        "\tcolumns.append(df)\n",
        "\tdf = pd.concat(columns, axis=1)\n",
        "\tdf.fillna(0, inplace=True)\n",
        "\treturn df\n",
        "\n",
        "# transform data to be supervised learning\n",
        "supervised = timeseries_to_supervised(diff_values, 1)\n",
        "supervised_values = supervised.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJijdExZyoNM"
      },
      "source": [
        "### Training-Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy1wVBKIyUi1"
      },
      "source": [
        "# split data into train and test-sets\n",
        "train, test = supervised_values[0:-12], supervised_values[-12:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIfQllUEzGk3"
      },
      "source": [
        "### Scale dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-UoibC3y_Y-"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scale train and test data to [-1, 1]\n",
        "def scale(train, test):\n",
        "\t# fit scaler\n",
        "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\tscaler = scaler.fit(train)\n",
        "\t# transform train\n",
        "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
        "\ttrain_scaled = scaler.transform(train)\n",
        "\t# transform test\n",
        "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
        "\ttest_scaled = scaler.transform(test)\n",
        "\treturn scaler, train_scaled, test_scaled\n",
        "  \n",
        "# transform the scale of the data\n",
        "scaler, train_scaled, test_scaled = scale(train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swVJQU6KzQn2"
      },
      "source": [
        "### Train LTSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJXmp4bYzL_M"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
        "\tX, y = train[:, 0:-1], train[:, -1]\n",
        "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
        "\tmodel.add(Dense(1))\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\tfor i in range(nb_epoch):\n",
        "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "\t\tmodel.reset_states()\n",
        "\treturn model\n",
        "\n",
        "# fit the model\n",
        "lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
        "\n",
        "# forecast the entire training dataset to build up state for forecasting\n",
        "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
        "lstm_model.predict(train_reshaped, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyOiSOEY0GGQ"
      },
      "source": [
        "### Evaluating the static LSTM model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oU6gVakzgDb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# make a one-step forecast\n",
        "def forecast_lstm(model, batch_size, X):\n",
        "\tX = X.reshape(1, 1, len(X))\n",
        "\tyhat = model.predict(X, batch_size=batch_size)\n",
        "\treturn yhat[0,0]\n",
        "\n",
        "# inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "\tnew_row = [x for x in X] + [value]\n",
        "\tarray = np.array(new_row)\n",
        "\tarray = array.reshape(1, len(array))\n",
        "\tinverted = scaler.inverse_transform(array)\n",
        "\treturn inverted[0, -1]\n",
        "\n",
        "# walk-forward validation on the test data\n",
        "predictions = list()\n",
        "for i in range(len(test_scaled)):\n",
        "\t# make one-step forecast\n",
        "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
        "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
        "\t\n",
        "  # invert scaling\n",
        "\tyhat = invert_scale(scaler, X, yhat)\n",
        "\t\n",
        "  # invert differencing\n",
        "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "\t\n",
        "  # store forecast\n",
        "\tpredictions.append(yhat)\n",
        "\texpected = raw_values[len(train) + i + 1]\n",
        "\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knbk-F6P0WCW"
      },
      "source": [
        "### Performance of the forecasts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlQ3hqR50UKS"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# report performance\n",
        "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "\n",
        "# line plot of observed vs predicted\n",
        "pyplot.plot(raw_values[-12:])\n",
        "pyplot.plot(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8AR67nv3iBQ"
      },
      "source": [
        "## Develop a Robust Result\n",
        "\n",
        "A difficulty with neural networks is that they give different results with different starting conditions.\n",
        "\n",
        "One approach might be to fix the random number seed used by Keras to ensure the results are reproducible. Another approach would be to control for the random initial conditions using a different experimental setup.\n",
        "\n",
        "We can repeat the experiment from the previous section multiple times, then take the average RMSE as an indication of how well the configuration would be expected to perform on unseen data on average.\n",
        "\n",
        "This is often called multiple repeats or multiple restarts.\n",
        "\n",
        "We can wrap the model fitting and walk-forward validation in a loop of fixed number of repeats. Each iteration the RMSE of the run can be recorded. We can then summarise the distribution of RMSE scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoTFYjv34Yk"
      },
      "source": [
        "# repeat experiment\n",
        "repeats = 10\n",
        "\n",
        "error_scores = list()\n",
        "for r in range(repeats):\n",
        "  # fit the model\n",
        "  lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
        "  # forecast the entire training dataset to build up state for forecasting\n",
        "  train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
        "  lstm_model.predict(train_reshaped, batch_size=1)\n",
        "\t\n",
        "  # walk-forward validation on the test data\n",
        "  predictions = list()\n",
        "  \n",
        "  for i in range(len(test_scaled)):\n",
        "    # make one-step forecast\n",
        "    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
        "    yhat = forecast_lstm(lstm_model, 1, X)\n",
        "    # invert scaling\n",
        "    yhat = invert_scale(scaler, X, yhat)\n",
        "    # invert differencing\n",
        "    yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "    # store forecast\n",
        "    predictions.append(yhat)\n",
        "\t\n",
        "  # report performance\n",
        "  rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
        "  print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
        "  error_scores.append(rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOsJUQtV6qSv"
      },
      "source": [
        "### Visualise performance summary\n",
        "\n",
        "Running the example prints the RMSE score each repeat. The end of the run provides summary statistics of the collected RMSE scores.\n",
        "\n",
        "**Note**: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "This is a very useful result as it suggests the result reported was probably a statistical fluke. The experiment suggests that the model is probably about as good as the persistence model on average, if not slightly worse.\n",
        "\n",
        "This indicates that, at the very least, further model tuning is required.\n",
        "\n",
        "A box and whisker plot captures the middle of the data as well as the extents and outlier results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS13Um4c4kL9"
      },
      "source": [
        "# Summarise results\n",
        "results = pd.DataFrame()\n",
        "results['rmse'] = error_scores\n",
        "print(results.describe())\n",
        "results.boxplot()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}